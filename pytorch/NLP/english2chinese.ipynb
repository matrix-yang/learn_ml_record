{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderGru(torch.nn.Module):\n",
    "    def __init__(self,word_size,hidden_size):\n",
    "        super(EncoderGru,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding_layer=torch.nn.Embedding(word_size,hidden_size)\n",
    "        self.gru=torch.nn.GRU(hidden_size,hidden_size)\n",
    "    \n",
    "    def forward(self,input_vector,hidden):\n",
    "        embedded=self.embedding_layer(input_vector)\n",
    "        embedded=embedded.unsqueeze(1)\n",
    "        out,hid=self.gru(embedded,hidden)\n",
    "        return out,hid\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttenDecoder(torch.nn.Module):\n",
    "    def __init__(self,word_szie,hidden_size):\n",
    "        super(AttenDecoder,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding_layer=torch.nn.Embedding(word_szie,hidden_size)\n",
    "        self.atten_layer=torch.nn.Linear(hidden_size*2,50)\n",
    "        self.atten_combine_layer=torch.nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.gru=torch.nn.GRU(hidden_size,hidden_size)\n",
    "        self.last_layer=torch.nn.Linear(hidden_size,word_szie)\n",
    "        \n",
    "    def forward(self,input_vector,hidden,encoder_output):\n",
    "        embeded=self.embedding_layer(input_vector)\n",
    "        contacted=torch.cat((embeded,hidden[0]),dim=1)\n",
    "        atten=self.atten_layer(contacted)\n",
    "        atten=torch.nn.functional.softmax(atten,dim=1)\n",
    "        atten_apply=torch.mm(atten,encoder_output.view(-1,256))\n",
    "        \n",
    "        atten_in=torch.cat((embeded,atten_apply),dim=1)\n",
    "        \n",
    "        gru_in=self.atten_combine_layer(atten_in)\n",
    "        atten=torch.nn.functional.relu(atten)\n",
    "        gru_in=gru_in.unsqueeze(0)\n",
    "        \n",
    "        out,hid=self.gru(gru_in,hidden)\n",
    "        \n",
    "        out=self.last_layer(out[0])\n",
    "        return out,hid,atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs(path='F:/Github/machine_learn_record/pytorch/data/cmn.txt'):\n",
    "    file=open(path,encoding='utf-8')\n",
    "    content=file.read()\n",
    "    pairs=[]\n",
    "    for p in content.split('\\n'):\n",
    "        temp=p.split('\\t')\n",
    "        pairs.append(temp)\n",
    "        \n",
    "    #调整英语中的符号\n",
    "    for p in pairs:\n",
    "        es=p[0].lower().strip()\n",
    "        es= re.sub(r\"([.!?])\", r\" \\1\", es)\n",
    "        es = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", es)\n",
    "        p[0]=es\n",
    "    #删除最后一行\n",
    "    pairs.pop()\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs=read_pairs()\n",
    "pairs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self,name):\n",
    "        self.name=name\n",
    "        self.index2word={}\n",
    "        self.word2index={0: \"SOS\", 1: \"EOS\"}\n",
    "        self.word2count={}\n",
    "        self.n_word=2\n",
    "    \n",
    "    def add_sentence(self,sentence):\n",
    "        if self.name=='en':\n",
    "            for w in sentence.split(' '):\n",
    "                self.add_word(w)\n",
    "        else:\n",
    "            for w in sentence:\n",
    "                self.add_word(w)\n",
    "                \n",
    "    def add_word(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.index2word[self.n_word]=word\n",
    "            self.word2index[word]=self.n_word\n",
    "            self.word2count[word]=1\n",
    "            self.n_word+=1\n",
    "        else:\n",
    "            self.word2count[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese=Lang('cn')\n",
    "english=Lang('en')\n",
    "for p in pairs:\n",
    "    chinese.add_sentence(p[1])\n",
    "    english.add_sentence(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2tensor(sentence,lang,device=torch.device(\"cuda\")):\n",
    "    idxs=[]\n",
    "    if lang.name=='en':\n",
    "        for w in sentence.split(' '):\n",
    "            idxs.append(lang.word2index[w])\n",
    "    else:\n",
    "        for w in sentence:\n",
    "            idxs.append(lang.word2index[w])\n",
    "    idxs.append(1)\n",
    "    tnr=torch.tensor(idxs,dtype=torch.long)\n",
    "    return tnr.view(-1,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,encoder_optimizer,decoder_optimizer,loss_f,inputs,outs):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    encoder_outs=torch.zeros((50,1,1,256),dtype=torch.float).to(device)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        encoder_outs[i],hidden=encoder.forward(inputs[i],encoder.init_hidden().to(device))\n",
    "\n",
    "    loss=0\n",
    "    \n",
    "    decoder_in=torch.tensor([0],dtype=torch.long).to(device)\n",
    "    for i in range(outs.shape[0]):\n",
    "        out,hidden,atten=atten_decorder.forward(decoder_in,hidden,encoder_outs)\n",
    "        topv, topi = out.data.topk(1)\n",
    "        decoder_in=topi.squeeze(0).detach() if random.random()>0.5 else outs[i]\n",
    "        temp=loss_f(out,outs[i])\n",
    "        loss+=temp\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item()/outs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(encoder,atten_decorder):\n",
    "    encoder_optimizer=torch.optim.Adam(encoder.parameters(),lr=0.0001)\n",
    "    decoder_optimizer=torch.optim.Adam(atten_decorder.parameters(),lr=0.0001)\n",
    "    loss_f=torch.nn.CrossEntropyLoss()\n",
    "    datas=[random.choice(pairs) for i in range(15000)]\n",
    "    loss1000=0\n",
    "    for i,p in enumerate(datas):\n",
    "        inputs= sentence2tensor(p[0],english)\n",
    "        outs= sentence2tensor(p[1],chinese)\n",
    "        loss=train(encoder,atten_decorder,encoder_optimizer,decoder_optimizer,loss_f,inputs,outs)\n",
    "        loss1000+=loss\n",
    "        if i%100==0:\n",
    "            print('{} epoc avg loss is {}'.format(i,loss1000/100))\n",
    "            loss1000=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder=EncoderGru(english.n_word,256)\n",
    "atten_decorder=AttenDecoder(chinese.n_word,256)\n",
    "encoder.to(device)\n",
    "atten_decorder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    model_train(encoder,atten_decorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(p,ecncoder,atten_decorder):\n",
    "    inputs= sentence2tensor(p[0],english)\n",
    "    outs= sentence2tensor(p[1],chinese)\n",
    "    encoder_outs=torch.zeros((50,1,1,256),dtype=torch.float).to(device)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        encoder_outs[i],hidden=encoder.forward(inputs[i],encoder.init_hidden().to(device))\n",
    "\n",
    "    decoder_in=torch.tensor([0],dtype=torch.long).to(device)\n",
    "    rs=[]\n",
    "    attens=[]\n",
    "    for i in range(50):\n",
    "        out,hidden,atten=atten_decorder.forward(decoder_in,hidden,encoder_outs)\n",
    "        topv, topi = out.topk(1)\n",
    "        decoder_in=topi.squeeze(0).detach()\n",
    "        if topi[0].item()==1:\n",
    "            break\n",
    "        attens.append(atten.cpu().detach().numpy()[0])\n",
    "        rs.append(chinese.index2word[topi[0].item()])\n",
    "    return rs,np.asarray(attens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    a=random.randint(0,10000)\n",
    "    rs,atten=predict(pairs[a],encoder,atten_decorder)\n",
    "    showAttention(pairs[a][0],rs,atten[:,:len(pairs[a][0].split(' '))])\n",
    "    print(\"input: {}\\ntarget: {}\\npredict: {}\".format(pairs[a][0],pairs[a][1],''.join(rs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence,ecncoder,atten_decorder):\n",
    "    inputs= sentence2tensor(sentence,english)\n",
    "    encoder_outs=torch.zeros((50,1,1,256),dtype=torch.float).to(device)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        encoder_outs[i],hidden=encoder.forward(inputs[i],encoder.init_hidden().to(device))\n",
    "\n",
    "    decoder_in=torch.tensor([0],dtype=torch.long).to(device)\n",
    "    rs=[]\n",
    "    for i in range(50):\n",
    "        out,hidden,atten=atten_decorder.forward(decoder_in,hidden,encoder_outs)\n",
    "        topv, topi = out.data.topk(1)\n",
    "        decoder_in=topi.squeeze(0).detach()\n",
    "        if topi[0].item()==1:\n",
    "            break\n",
    "        rs.append(chinese.index2word[topi[0].item()])\n",
    "    print(\"in------>\",sentence,\"predict--->\",\"\".join(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in------> tom dead . predict---> 汤姆死了。\n"
     ]
    }
   ],
   "source": [
    "translate('tom dead .',encoder,atten_decorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
