{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderGru(torch.nn.Module):\n",
    "    def __init__(self,word_size,hidden_size):\n",
    "        super(EncoderGru,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding_layer=torch.nn.Embedding(word_size,hidden_size)\n",
    "        self.gru=torch.nn.GRU(hidden_size,hidden_size)\n",
    "    \n",
    "    def forward(self,input_vector,hidden):\n",
    "        embedded=self.embedding_layer(input_vector)\n",
    "        embedded=embedded.unsqueeze(1)\n",
    "        out,hid=self.gru(embedded,hidden)\n",
    "        return out,hid\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttenDecoder(torch.nn.Module):\n",
    "    def __init__(self,word_szie,hidden_size):\n",
    "        super(AttenDecoder,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding_layer=torch.nn.Embedding(word_szie,hidden_size)\n",
    "        self.atten_layer=torch.nn.Linear(hidden_size*2,50)\n",
    "        self.atten_combine_layer=torch.nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.gru=torch.nn.GRU(hidden_size,hidden_size)\n",
    "        self.last_layer=torch.nn.Linear(hidden_size,word_szie)\n",
    "        \n",
    "    def forward(self,input_vector,hidden,encoder_output):\n",
    "        embeded=self.embedding_layer(input_vector)\n",
    "        contacted=torch.cat((embeded,hidden[0]),dim=1)\n",
    "        atten=self.atten_layer(contacted)\n",
    "        atten_apply=torch.mm(atten,encoder_output.view(-1,256))\n",
    "        \n",
    "        atten_in=torch.cat((embeded,atten_apply),dim=1)\n",
    "        \n",
    "        gru_in=self.atten_combine_layer(atten_in)\n",
    "        gru_in=gru_in.unsqueeze(0)\n",
    "        \n",
    "        out,hid=self.gru(gru_in,hidden)\n",
    "        \n",
    "        out=self.last_layer(out[0])\n",
    "        return out,hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs(path='F:/Github/machine_learn_record/pytorch/data/cmn.txt'):\n",
    "    file=open(path,encoding='utf-8')\n",
    "    content=file.read()\n",
    "    pairs=[]\n",
    "    for p in content.split('\\n'):\n",
    "        temp=p.split('\\t')\n",
    "        pairs.append(temp)\n",
    "        \n",
    "    #调整英语中的符号\n",
    "    for p in pairs:\n",
    "        es=p[0].lower().strip()\n",
    "        es= re.sub(r\"([.!?])\", r\" \\1\", es)\n",
    "        es = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", es)\n",
    "        p[0]=es\n",
    "    #删除最后一行\n",
    "    pairs.pop()\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs=read_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self,name):\n",
    "        self.name=name\n",
    "        self.index2word={}\n",
    "        self.word2index={0: \"SOS\", 1: \"EOS\"}\n",
    "        self.word2count={}\n",
    "        self.n_word=2\n",
    "    \n",
    "    def add_sentence(self,sentence):\n",
    "        if self.name=='en':\n",
    "            for w in sentence.split(' '):\n",
    "                self.add_word(w)\n",
    "        else:\n",
    "            for w in sentence:\n",
    "                self.add_word(w)\n",
    "                \n",
    "    def add_word(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.index2word[self.n_word]=word\n",
    "            self.word2index[word]=self.n_word\n",
    "            self.word2count[word]=1\n",
    "            self.n_word+=1\n",
    "        else:\n",
    "            self.word2count[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese=Lang('cn')\n",
    "english=Lang('en')\n",
    "for p in pairs:\n",
    "    chinese.add_sentence(p[1])\n",
    "    english.add_sentence(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2tensor(sentence,lang,device=torch.device(\"cuda\")):\n",
    "    idxs=[]\n",
    "    if lang.name=='en':\n",
    "        for w in sentence.split(' '):\n",
    "            idxs.append(lang.word2index[w])\n",
    "    else:\n",
    "        for w in sentence:\n",
    "            idxs.append(lang.word2index[w])\n",
    "    idxs.append(1)\n",
    "    tnr=torch.tensor(idxs,dtype=torch.long)\n",
    "    return tnr.view(-1,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,encoder_optimizer,decoder_optimizer,loss_f,inputs,outs):\n",
    "    encoder_outs=torch.zeros((50,1,1,256),dtype=torch.float).to(device)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        encoder_outs[i],hidden=encoder.forward(inputs[i],encoder.init_hidden().to(device))\n",
    "\n",
    "    loss=0\n",
    "    \n",
    "    decoder_in=torch.tensor([0],dtype=torch.long).to(device)\n",
    "    for i in range(outs.shape[0]):\n",
    "        out,hidden=atten_decorder.forward(decoder_in,hidden,encoder_outs)\n",
    "        topv, topi = out.data.topk(1)\n",
    "        decoder_in=topi.squeeze(0).detach() if random.random()>0.5 else outs[i]\n",
    "        loss+=loss_f(out,outs[i])\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item()/outs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(encoder,atten_decorder):\n",
    "    encoder_optimizer=torch.optim.SGD(ecncoder.parameters(),lr=0.001)\n",
    "    decoder_optimizer=torch.optim.SGD(atten_decorder.parameters(),lr=0.001)\n",
    "    loss_f=torch.nn.CrossEntropyLoss()\n",
    "    datas=[random.choice(pairs) for i in range(15000)]\n",
    "    loss1000=0\n",
    "    for i,p in enumerate(datas):\n",
    "        inputs= sentence2tensor(p[0],english)\n",
    "        outs= sentence2tensor(p[1],chinese)\n",
    "        loss=train(encoder,atten_decorder,encoder_optimizer,decoder_optimizer,loss_f,inputs,outs)\n",
    "        loss1000+=loss\n",
    "        print(loss1000)\n",
    "        if i%1000==999:\n",
    "            print('{} epoc avg loss is {}'.format(i,loss1000/1000))\n",
    "            loss1000=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttenDecoder(\n",
       "  (embedding_layer): Embedding(3439, 256)\n",
       "  (atten_layer): Linear(in_features=512, out_features=50, bias=True)\n",
       "  (atten_combine_layer): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (gru): GRU(256, 256)\n",
       "  (last_layer): Linear(in_features=256, out_features=3439, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder=EncoderGru(english.n_word,256)\n",
    "atten_decorder=AttenDecoder(chinese.n_word,256)\n",
    "encoder.to(device)\n",
    "atten_decorder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 epoc avg loss is 2714.9952819870914\n",
      "1999 epoc avg loss is 2892.429995227221\n",
      "2999 epoc avg loss is 3074.048898804599\n",
      "3999 epoc avg loss is 3255.610831805999\n",
      "4999 epoc avg loss is 3465.2557185724004\n",
      "5999 epoc avg loss is 3646.1713126738805\n",
      "6999 epoc avg loss is 3787.6926430907843\n",
      "7999 epoc avg loss is 3983.5497674811104\n",
      "8999 epoc avg loss is 4100.21869844775\n",
      "9999 epoc avg loss is 4326.269555655717\n",
      "10999 epoc avg loss is 4564.293657520129\n",
      "11999 epoc avg loss is 4758.90573353196\n",
      "12999 epoc avg loss is 4941.175668666164\n",
      "13999 epoc avg loss is 5126.419606660099\n",
      "14999 epoc avg loss is 5312.339017648663\n"
     ]
    }
   ],
   "source": [
    "model_train(encoder,atten_decorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成\n",
      "好\n",
      "上\n",
      "土\n",
      "skip it . 不管它。\n"
     ]
    }
   ],
   "source": [
    "def predict(p,ecncoder,atten_decorder):\n",
    "    inputs= sentence2tensor(p[0],english)\n",
    "    outs= sentence2tensor(p[1],chinese)\n",
    "    encoder_outs=torch.zeros((50,1,1,256),dtype=torch.float).to(device)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        encoder_outs[i],hidden=encoder.forward(inputs[i],encoder.init_hidden().to(device))\n",
    "\n",
    "    loss=0\n",
    "    decoder_in=torch.tensor([0],dtype=torch.long).to(device)\n",
    "    for i in range(outs.shape[0]):\n",
    "        out,hidden=atten_decorder.forward(decoder_in,hidden,encoder_outs)\n",
    "        topv, topi = out.data.topk(1)\n",
    "        decoder_in=topi.squeeze(0).detach()\n",
    "        print(chinese.index2word[topi[0].item()])\n",
    "    print(p[0],p[1])\n",
    "\n",
    "predict(pairs[56],ecncoder,atten_decorder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'收'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese.index2word[1070]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
