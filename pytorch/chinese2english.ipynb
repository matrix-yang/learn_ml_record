{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderGru(torch.nn.Module):\n",
    "    def __init__(self,word_size,hidden_size):\n",
    "        super(EncoderGru,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding_layer=torch.nn.Embedding(word_size,hidden_size)\n",
    "        self.gru=torch.nn.GRU(hidden_size,hidden_size)\n",
    "    \n",
    "    def forward(self,input_vector,hidden):\n",
    "        embedded=self.embedding_layer(input_vector)\n",
    "        embedded=embedded.unsqueeze(1)\n",
    "        out,hid=self.gru(embedded,hidden)\n",
    "        return out,hid\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttenDecoder(torch.nn.Module):\n",
    "    def __init__(self,word_szie,hidden_size):\n",
    "        super(AttenDecoder,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding_layer=torch.nn.Embedding(word_szie,hidden_size)\n",
    "        self.atten_layer=torch.nn.Linear(hidden_size*2,50)\n",
    "        self.atten_combine_layer=torch.nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.gru=torch.nn.GRU(hidden_size,hidden_size)\n",
    "        self.last_layer=torch.nn.Linear(hidden_size,word_szie)\n",
    "        \n",
    "    def forward(self,input_vector,hidden,encoder_output):\n",
    "        embeded=self.embedding_layer(input_vector)\n",
    "        contacted=torch.cat((embeded,hidden[0]),dim=1)\n",
    "        atten=self.atten_layer(contacted)\n",
    "        atten_apply=torch.mm(atten,encoder_output.view(-1,256))\n",
    "        \n",
    "        atten_in=torch.cat((embeded,atten_apply),dim=1)\n",
    "        \n",
    "        gru_in=self.atten_combine_layer(atten_in)\n",
    "        gru_in=gru_in.unsqueeze(0)\n",
    "        \n",
    "        out,hid=self.gru(gru_in,hidden)\n",
    "        \n",
    "        out=self.last_layer(out[0])\n",
    "        return out,hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs(path='F:/Github/machine_learn_record/pytorch/data/cmn.txt'):\n",
    "    file=open(path,encoding='utf-8')\n",
    "    content=file.read()\n",
    "    pairs=[]\n",
    "    for p in content.split('\\n'):\n",
    "        temp=p.split('\\t')\n",
    "        pairs.append(temp)\n",
    "        \n",
    "    #调整英语中的符号\n",
    "    for p in pairs:\n",
    "        es=p[0].lower().strip()\n",
    "        es= re.sub(r\"([.!?])\", r\" \\1\", es)\n",
    "        es = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", es)\n",
    "        p[0]=es\n",
    "    #删除最后一行\n",
    "    pairs.pop()\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi .', '你好。']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs=read_pairs()\n",
    "pairs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self,name):\n",
    "        self.name=name\n",
    "        self.index2word={}\n",
    "        self.word2index={0: \"SOS\", 1: \"EOS\"}\n",
    "        self.word2count={}\n",
    "        self.n_word=2\n",
    "    \n",
    "    def add_sentence(self,sentence):\n",
    "        if self.name=='en':\n",
    "            for w in sentence.split(' '):\n",
    "                self.add_word(w)\n",
    "        else:\n",
    "            for w in sentence:\n",
    "                self.add_word(w)\n",
    "                \n",
    "    def add_word(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.index2word[self.n_word]=word\n",
    "            self.word2index[word]=self.n_word\n",
    "            self.word2count[word]=1\n",
    "            self.n_word+=1\n",
    "        else:\n",
    "            self.word2count[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese=Lang('cn')\n",
    "english=Lang('en')\n",
    "for p in pairs:\n",
    "    chinese.add_sentence(p[1])\n",
    "    english.add_sentence(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2tensor(sentence,lang,device=torch.device(\"cuda\")):\n",
    "    idxs=[]\n",
    "    if lang.name=='en':\n",
    "        for w in sentence.split(' '):\n",
    "            idxs.append(lang.word2index[w])\n",
    "    else:\n",
    "        for w in sentence:\n",
    "            idxs.append(lang.word2index[w])\n",
    "    idxs.append(1)\n",
    "    tnr=torch.tensor(idxs,dtype=torch.long)\n",
    "    return tnr.view(-1,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4],\n",
       "        [5],\n",
       "        [3],\n",
       "        [1]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2tensor(pairs[1][1],chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,encoder_optimizer,decoder_optimizer,loss_f,inputs,outs):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    encoder_outs=torch.zeros((50,1,1,256),dtype=torch.float).to(device)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        encoder_outs[i],hidden=encoder.forward(inputs[i],encoder.init_hidden().to(device))\n",
    "\n",
    "    loss=0\n",
    "    \n",
    "    decoder_in=torch.tensor([0],dtype=torch.long).to(device)\n",
    "    for i in range(outs.shape[0]):\n",
    "        out,hidden=atten_decorder.forward(decoder_in,hidden,encoder_outs)\n",
    "        topv, topi = out.data.topk(1)\n",
    "        decoder_in=topi.squeeze(0).detach() if random.random()>0.5 else outs[i]\n",
    "        temp=loss_f(out,outs[i])\n",
    "        loss+=temp\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item()/outs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(encoder,atten_decorder):\n",
    "    encoder_optimizer=torch.optim.Adam(encoder.parameters(),lr=0.0001)\n",
    "    decoder_optimizer=torch.optim.Adam(atten_decorder.parameters(),lr=0.0001)\n",
    "    loss_f=torch.nn.CrossEntropyLoss()\n",
    "    datas=[random.choice(pairs) for i in range(15000)]\n",
    "    loss1000=0\n",
    "    for i,p in enumerate(datas):\n",
    "        inputs= sentence2tensor(p[0],english)\n",
    "        outs= sentence2tensor(p[1],chinese)\n",
    "        loss=train(encoder,atten_decorder,encoder_optimizer,decoder_optimizer,loss_f,inputs,outs)\n",
    "        loss1000+=loss\n",
    "        if i%100==0:\n",
    "            print('{} epoc avg loss is {}'.format(i,loss1000/100))\n",
    "            loss1000=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttenDecoder(\n",
       "  (embedding_layer): Embedding(3439, 256)\n",
       "  (atten_layer): Linear(in_features=512, out_features=50, bias=True)\n",
       "  (atten_combine_layer): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (gru): GRU(256, 256)\n",
       "  (last_layer): Linear(in_features=256, out_features=3439, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder=EncoderGru(english.n_word,256)\n",
    "atten_decorder=AttenDecoder(chinese.n_word,256)\n",
    "encoder.to(device)\n",
    "atten_decorder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoc avg loss is 0.021716778495094995\n",
      "100 epoc avg loss is 2.78499424045678\n",
      "200 epoc avg loss is 2.6096014312828157\n",
      "300 epoc avg loss is 2.84956206914461\n",
      "400 epoc avg loss is 2.971385979074071\n",
      "500 epoc avg loss is 2.689510003678125\n",
      "600 epoc avg loss is 2.921340236159592\n",
      "700 epoc avg loss is 2.7845103598009144\n",
      "800 epoc avg loss is 2.5861078882851594\n",
      "900 epoc avg loss is 2.8093954508984496\n",
      "1000 epoc avg loss is 2.74596874925988\n",
      "1100 epoc avg loss is 2.8888507730963973\n",
      "1200 epoc avg loss is 2.9403924292130186\n",
      "1300 epoc avg loss is 2.6315302677431394\n",
      "1400 epoc avg loss is 2.8970615227239485\n",
      "1500 epoc avg loss is 2.721724507754051\n",
      "1600 epoc avg loss is 2.755487748127667\n",
      "1700 epoc avg loss is 2.805469996816364\n",
      "1800 epoc avg loss is 2.790406708165844\n",
      "1900 epoc avg loss is 2.9991978410590203\n",
      "2000 epoc avg loss is 2.7284819810766137\n",
      "2100 epoc avg loss is 3.003541123858568\n",
      "2200 epoc avg loss is 3.008586110763167\n",
      "2300 epoc avg loss is 2.638329825702061\n",
      "2400 epoc avg loss is 3.0474701714925225\n",
      "2500 epoc avg loss is 2.7996552689255356\n",
      "2600 epoc avg loss is 2.739189884571721\n",
      "2700 epoc avg loss is 2.904912260123632\n",
      "2800 epoc avg loss is 2.8523120440106506\n",
      "2900 epoc avg loss is 2.795284411801026\n",
      "3000 epoc avg loss is 2.7788948254597834\n",
      "3100 epoc avg loss is 2.727724117500123\n",
      "3200 epoc avg loss is 2.634336022003662\n",
      "3300 epoc avg loss is 2.673743002686478\n",
      "3400 epoc avg loss is 2.9377136511781083\n",
      "3500 epoc avg loss is 2.878459162717391\n",
      "3600 epoc avg loss is 2.7585056641832426\n",
      "3700 epoc avg loss is 2.8823821104793392\n",
      "3800 epoc avg loss is 2.867215972059349\n",
      "3900 epoc avg loss is 2.671003743908726\n",
      "4000 epoc avg loss is 2.817655637452902\n",
      "4100 epoc avg loss is 2.6742101016696553\n",
      "4200 epoc avg loss is 2.781290231035874\n",
      "4300 epoc avg loss is 2.7853808582356647\n",
      "4400 epoc avg loss is 2.6667502558442897\n",
      "4500 epoc avg loss is 2.769180561666589\n",
      "4600 epoc avg loss is 2.7378700495358625\n",
      "4700 epoc avg loss is 2.9968550321228804\n",
      "4800 epoc avg loss is 2.642284826171558\n",
      "4900 epoc avg loss is 2.8246600585379196\n",
      "5000 epoc avg loss is 2.8236528090992987\n",
      "5100 epoc avg loss is 2.7943503864438135\n",
      "5200 epoc avg loss is 2.7362509197063845\n",
      "5300 epoc avg loss is 2.863259410287503\n",
      "5400 epoc avg loss is 2.619989950772811\n",
      "5500 epoc avg loss is 2.868958698921284\n",
      "5600 epoc avg loss is 2.9229323631822255\n",
      "5700 epoc avg loss is 2.696377673062527\n",
      "5800 epoc avg loss is 2.759508696367393\n",
      "5900 epoc avg loss is 2.671967779443338\n",
      "6000 epoc avg loss is 2.623994342266885\n",
      "6100 epoc avg loss is 2.759829294877836\n",
      "6200 epoc avg loss is 2.5845770861415707\n",
      "6300 epoc avg loss is 2.761000489978622\n",
      "6400 epoc avg loss is 2.8078417092392827\n",
      "6500 epoc avg loss is 2.6892724574545577\n",
      "6600 epoc avg loss is 2.6297529867132017\n",
      "6700 epoc avg loss is 2.7365443336854334\n",
      "6800 epoc avg loss is 2.8279289707999378\n",
      "6900 epoc avg loss is 2.783397187565414\n",
      "7000 epoc avg loss is 2.7730462039030415\n",
      "7100 epoc avg loss is 2.6348581635310055\n",
      "7200 epoc avg loss is 2.7320382138781194\n",
      "7300 epoc avg loss is 2.6501816975146424\n",
      "7400 epoc avg loss is 2.7695144189761693\n",
      "7500 epoc avg loss is 2.6973233838897706\n",
      "7600 epoc avg loss is 2.7441907874008287\n",
      "7700 epoc avg loss is 2.6871535933523996\n",
      "7800 epoc avg loss is 2.6010786381001556\n",
      "7900 epoc avg loss is 2.805102595873499\n",
      "8000 epoc avg loss is 2.8585592979787355\n",
      "8100 epoc avg loss is 2.7787379338171747\n",
      "8200 epoc avg loss is 2.42352389667686\n",
      "8300 epoc avg loss is 2.8480970221471695\n",
      "8400 epoc avg loss is 2.90640623605387\n",
      "8500 epoc avg loss is 2.9218270090572145\n",
      "8600 epoc avg loss is 2.576683989056998\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    model_train(encoder,atten_decorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(p,ecncoder,atten_decorder):\n",
    "    inputs= sentence2tensor(p[0],english)\n",
    "    outs= sentence2tensor(p[1],chinese)\n",
    "    encoder_outs=torch.zeros((50,1,1,256),dtype=torch.float).to(device)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        encoder_outs[i],hidden=encoder.forward(inputs[i],encoder.init_hidden().to(device))\n",
    "\n",
    "    decoder_in=torch.tensor([0],dtype=torch.long).to(device)\n",
    "    rs=[]\n",
    "    for i in range(50):\n",
    "        out,hidden=atten_decorder.forward(decoder_in,hidden,encoder_outs)\n",
    "        topv, topi = out.data.topk(1)\n",
    "        decoder_in=topi.squeeze(0).detach()\n",
    "        if topi[0].item()==1:\n",
    "            break\n",
    "        rs.append(chinese.index2word[topi[0].item()])\n",
    "    print(\"in------>\",p[0],\"target------>\",p[1],\"predict--->\",\"\".join(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in------> turn left at the next corner . target------> 下一个街角左转。 predict---> 把下下一點鐘。\n",
      "in------> you must clear the table . target------> 你必须把桌子清理干净。 predict---> 你必须清桌桌子。\n",
      "in------> what s wrong with your dog ? target------> 你的狗怎麼了？ predict---> 你的狗了？\n",
      "in------> i ll remember you forever . target------> 我會永遠記住你的。 predict---> 我會得記得你的。\n",
      "in------> i have no knife to cut with . target------> 我沒有刀子可用來切。 predict---> 我有任何上子。\n",
      "in------> we re both reasonable people . target------> 我们是两个通情达理的人。 predict---> 我们是人人人。人。\n",
      "in------> we gladly accept your offer . target------> 我们很高兴接受你的提议。 predict---> 我们人接受你的的的意。\n",
      "in------> whose is it ? target------> 这是谁的？ predict---> 这是谁的？\n",
      "in------> there is a large supermarket . target------> 有一個大型超市。 predict---> 有一大大。\n",
      "in------> tom likes it hot . target------> 汤姆喜欢热的。 predict---> 汤姆喜歡熱。\n",
      "in------> does she like oranges ? target------> 她喜歡柳橙嗎？ predict---> 我喜歡橙橙嗎？\n",
      "in------> he sat on the bench . target------> 他坐在長凳上。 predict---> 他坐在地上。\n",
      "in------> when do you need it by ? target------> 您何時需要它？ predict---> 你什麼需要做？\n",
      "in------> who invented the telephone ? target------> 谁发明了电话？ predict---> 谁把话话？\n",
      "in------> you re a real friend . target------> 你是個真正的朋友。 predict---> 你是个朋友。\n",
      "in------> please speak more clearly . target------> 请说得更清楚些。 predict---> 請說你更多。\n",
      "in------> that s my dictionary . target------> 那是我的字典。 predict---> 那是我的字典。\n",
      "in------> my son doesn t obey me . target------> 我儿子不听我话。 predict---> 我儿子不不不不。\n",
      "in------> who told you the story ? target------> 誰告訴你這個故事？ predict---> 你告訴了這個故事？\n",
      "in------> why should it be different ? target------> 为什么它应该要变得不一样？ predict---> 为什么应该不应该做什麼？\n",
      "in------> you re bound to fail . target------> 你注定失败。 predict---> 你不定會定。\n",
      "in------> the snow was knee deep . target------> 積雪深及膝蓋。 predict---> 雪雪雪被了。\n",
      "in------> english is easy to learn . target------> 英語簡單易學。 predict---> 英語是大學習。\n",
      "in------> try to keep from crying . target------> 試著不要哭。 predict---> 再试试试试来。\n",
      "in------> she won everything . target------> 她贏得一切了。 predict---> 她會了一切。\n",
      "in------> you don t understand . target------> 你不了解。 predict---> 你不懂。\n",
      "in------> i made an abstract of a book . target------> 我為一本書寫了內容提要。 predict---> 我做一本書的書。\n",
      "in------> a stitch in time saves nine . target------> 防微杜漸。 predict---> 一次都睡觉得很。\n",
      "in------> i am amazed at your audacity . target------> 我对你的厚颜无耻感到惊讶。 predict---> 我對你的你的感到感到惊。。\n",
      "in------> spring is coming soon . target------> 春天就要来了。 predict---> 春天快就快。\n",
      "in------> my father s hobby is fishing . target------> 我父親的嗜好是釣魚。 predict---> 我父親是嗜是是的的。\n",
      "in------> i m going to study harder . target------> 我決定更努力讀書。 predict---> 我要去努力学习。\n",
      "in------> i m afraid to go alone . target------> 我害怕一個人去。 predict---> 我怕去人去。\n",
      "in------> do whatever he tells you . target------> 他跟你说什么，你就做什么。 predict---> 你知道你有什么。\n",
      "in------> i must be leaving now . target------> 我現在必須離開。 predict---> 我必須在在離開。\n",
      "in------> who buys this type of art ? target------> 谁买这种艺术品？ predict---> 谁买这个旅行？\n",
      "in------> they became close friends . target------> 他們成了密友。 predict---> 他們成為了朋友。\n",
      "in------> we have to pull the weeds . target------> 我們必須拔除雜草。 predict---> 我们必須須去了了了。\n",
      "in------> this is a low budget movie . target------> 這是一部低成本的電影。 predict---> 这是一個電影的電影。\n",
      "in------> i d better get back home . target------> 我回家比较好。 predict---> 我最好回回家。\n",
      "in------> i saw it with my own eyes . target------> 我親眼看見了。 predict---> 我看起看起己的眼睛。\n",
      "in------> may i introduce myself ? target------> 我可以自我介紹嗎? predict---> 我可以把我自己嗎？\n",
      "in------> the rumor proved to be true . target------> 经过证实，谣言是真的。 predict---> 这个消消息是真是真的。\n",
      "in------> i m not tired at all . target------> 我根本不累。 predict---> 我不累不累。\n",
      "in------> try resting for now . target------> 现在休息一下。 predict---> 在个现在了。\n",
      "in------> i found this book interesting . target------> 我觉得这本书很有意思。 predict---> 我覺得這本书很有趣。\n",
      "in------> why am i still here ? target------> 为什么我还在这里？ predict---> 你為什麼還在在在这儿？\n",
      "in------> my passport has expired . target------> 我的護照過期了。 predict---> 我的照照過了。\n",
      "in------> you need a vacation . target------> 你需要個假期。 predict---> 你需要假。\n",
      "in------> she gave me a watch . target------> 她給了我一支手錶。 predict---> 她給了我一手。\n",
      "in------> i m going to shoot him dead . target------> 我要（用枪）打死他。 predict---> 我要去他他。\n",
      "in------> you and tom must be happy . target------> 你和湯姆一定很高興。 predict---> 你和湯姆必須必要一定。\n",
      "in------> tom is boring . target------> 汤姆是个无聊的人。 predict---> 汤姆是个人。\n",
      "in------> she made a mess of the work . target------> 她工作做得亂七八糟。 predict---> 她做他工作作。\n",
      "in------> this is a dangerous mission . target------> 這是個危險的任務。 predict---> 这是危險的的。\n",
      "in------> please be quiet . target------> 請安靜。 predict---> 請安靜。\n",
      "in------> don t release that prisoner . target------> 別放了那個犯人。 predict---> 别要那那个犯。。\n",
      "in------> he s a big boy . target------> 他是個大男孩。 predict---> 他是个大大的男孩。\n",
      "in------> please spell your name . target------> 请拼一下您的名字。 predict---> 請你你的名字。\n",
      "in------> get down ! target------> 趴下！ predict---> 把！！\n",
      "in------> he climbed mt . fuji . target------> 他爬上了富士山。 predict---> 他爬士山山。\n",
      "in------> are you going out tomorrow ? target------> 你明天出去吗？ predict---> 你要明出去嗎？\n",
      "in------> can you save tom ? target------> 你能救汤姆吗？ predict---> 你能跟汤姆姆嗎？\n",
      "in------> what was that you just said ? target------> 你刚说了什么？ predict---> 你说是说话？\n",
      "in------> of course . target------> 当然了。 predict---> 不要。\n",
      "in------> it s hot today isn t it ? target------> 今天很熱，不是嗎? predict---> 今天天不是天，不是嗎？\n",
      "in------> the rumor isn t true . target------> 傳聞不是正確的。 predict---> 这个太不是真的。\n",
      "in------> i don t like her . target------> 我不喜歡她。 predict---> 我不喜欢她。\n",
      "in------> aren t you tom ? target------> 你不是湯姆嗎？ predict---> 你不是湯姆嗎？\n",
      "in------> how do you spell your name ? target------> 你怎麼拼你的名字的？ predict---> 你怎麼樣的名字？\n",
      "in------> would you like to come ? target------> 你愿意来吗？ predict---> 你想來來嗎？\n",
      "in------> let s drive to the lake . target------> 讓我們開車到湖邊。 predict---> 讓我們開車開車。\n",
      "in------> come on ! give me a chance . target------> 来嘛！给我个机会。 predict---> 你！我一！\n",
      "in------> i m new at this kind of work . target------> 我新接触這種工作。 predict---> 我在這本工作的事。\n",
      "in------> i have no secrets from you . target------> 我對你毫無隱瞞。 predict---> 我不不再来。\n",
      "in------> i m thrilled . target------> 我激動不已。 predict---> 我很快。\n",
      "in------> i don t have any sisters . target------> 我没有姐妹。 predict---> 我不有任何。\n",
      "in------> we had a lot of furniture . target------> 我們有很多家具。 predict---> 我們有很多多。\n",
      "in------> i d better get back home . target------> 我回家比较好。 predict---> 我最好回回家。\n",
      "in------> tom is happy . target------> 湯姆高興。 predict---> 汤姆很高兴。\n",
      "in------> what are you talking about ? target------> 你说的是什么？ predict---> 你們在什麼什麼？\n",
      "in------> what are you doing tonight ? target------> 你今晚在做什麼？ predict---> 你今晚在做什麼？\n",
      "in------> it s a nice day . target------> 今天天氣很好。 predict---> 它是一天。\n",
      "in------> he is very brave . target------> 他很勇敢。 predict---> 他很很。。\n",
      "in------> please turn off the tv . target------> 請把電視關掉。 predict---> 請關電視。\n",
      "in------> i might say yes . target------> 我可能会说是。 predict---> 我可能會說。\n",
      "in------> i ve been on vacation . target------> 我在休假。 predict---> 我在假。\n",
      "in------> i visit him every other day . target------> 我每隔一天拜訪他。 predict---> 我每天他每天他。\n",
      "in------> she saw a tall man yesterday . target------> 她昨天看见一个高大的男人。 predict---> 她昨天看看看高一个人。\n",
      "in------> you think i m mad don t you ? target------> 你認為我瘋了，不是麼？ predict---> 你認為我是不是不是不是不是？\n",
      "in------> what are you called ? target------> 你們叫什麼名字？ predict---> 你們叫什么？\n",
      "in------> i d like some tea please . target------> 請給我一杯茶。 predict---> 我想要一茶。\n",
      "in------> i wish you d slow down . target------> 我希望你慢下來。 predict---> 我希望你下。\n",
      "in------> why do you need my help ? target------> 你為甚麼需要我幫助？ predict---> 你為什麼需要我的幫助？\n",
      "in------> i m really scared of spiders . target------> 我真怕蜘蛛。 predict---> 我真怕心。\n",
      "in------> they should be scared . target------> 他们应该会被吓到。 predict---> 他們应该要害怕。\n",
      "in------> how did your weekend go ? target------> 你的周末是怎么过的？ predict---> 你怎末末去？\n",
      "in------> how about you ? target------> 你怎麼樣？ predict---> 你們怎麼？\n",
      "in------> i agree with that opinion . target------> 我同意那個意見。 predict---> 我同意那那个意。。\n",
      "in------> that child has few friends . target------> 那孩子沒有什麼朋友。 predict---> 那孩子有有有有朋友。\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    a=random.randint(0,10000)\n",
    "    predict(pairs[a],encoder,atten_decorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence,ecncoder,atten_decorder):\n",
    "    inputs= sentence2tensor(sentence,english)\n",
    "    encoder_outs=torch.zeros((50,1,1,256),dtype=torch.float).to(device)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        encoder_outs[i],hidden=encoder.forward(inputs[i],encoder.init_hidden().to(device))\n",
    "\n",
    "    decoder_in=torch.tensor([0],dtype=torch.long).to(device)\n",
    "    rs=[]\n",
    "    for i in range(50):\n",
    "        out,hidden=atten_decorder.forward(decoder_in,hidden,encoder_outs)\n",
    "        topv, topi = out.data.topk(1)\n",
    "        decoder_in=topi.squeeze(0).detach()\n",
    "        if topi[0].item()==1:\n",
    "            break\n",
    "        rs.append(chinese.index2word[topi[0].item()])\n",
    "    print(\"in------>\",sentence,\"predict--->\",\"\".join(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in------> i like baby . predict---> 我喜歡了。\n"
     ]
    }
   ],
   "source": [
    "translate('i like baby .',encoder,atten_decorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
