{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderGru(torch.nn.Module):\n",
    "    def __init__(self,word_size,hidden_size):\n",
    "        super(EncoderGru,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding_layer=torch.nn.Embedding(word_size,hidden_size)\n",
    "        self.gru=torch.nn.GRU(hidden_size,hidden_size)\n",
    "    \n",
    "    def forward(self,input_vector,hidden):\n",
    "        embedded=self.embedding_layer(input_vector)\n",
    "        embedded=embedded.unsqueeze(1)\n",
    "        out,hid=self.gru(embedded,hidden)\n",
    "        return out,hid\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttenDecoder(torch.nn.Module):\n",
    "    def __init__(self,word_szie,hidden_size):\n",
    "        super(AttenDecoder,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding_layer=torch.nn.Embedding(word_szie,hidden_size)\n",
    "        self.atten_layer=torch.nn.Linear(hidden_size*2,50)\n",
    "        self.atten_combine_layer=torch.nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.gru=torch.nn.GRU(hidden_size,hidden_size)\n",
    "        self.last_layer=torch.nn.Linear(hidden_size,word_szie)\n",
    "        \n",
    "    def forward(self,input_vector,hidden,encoder_output):\n",
    "        embeded=self.embedding_layer(input_vector)\n",
    "        contacted=torch.cat((embeded,hidden[0]),dim=1)\n",
    "        atten=self.atten_layer(contacted)\n",
    "        atten_apply=torch.mm(atten,encoder_output.view(-1,4))\n",
    "        \n",
    "        atten_in=torch.cat((embeded,atten_apply),dim=1)\n",
    "        \n",
    "        gru_in=self.atten_combine_layer(atten_in)\n",
    "        gru_in=gru_in.unsqueeze(0)\n",
    "        \n",
    "        out,hid=self.gru(gru_in,hidden)\n",
    "        \n",
    "        out=self.last_layer(out[0])\n",
    "        return out,hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecncoder=EncoderGru(english.n_word,4)\n",
    "atten_decorder=AttenDecoder(chinese.n_word,4)\n",
    "encoder_optimizer=torch.optim.SGD(ecncoder.parameters(),lr=0.001)\n",
    "decoder_optimizer=torch.optim.SGD(atten_decorder.parameters(),lr=0.001)\n",
    "loss_f=torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 187],\n",
      "        [  62],\n",
      "        [  41],\n",
      "        [  15],\n",
      "        [2192],\n",
      "        [  16]])\n",
      "torch.Size([1, 4]) torch.Size([1, 4])\n",
      "torch.Size([1, 4]) torch.Size([1, 4])\n",
      "torch.Size([1, 4]) torch.Size([1, 4])\n",
      "torch.Size([1, 4]) torch.Size([1, 4])\n",
      "torch.Size([1, 4]) torch.Size([1, 4])\n",
      "torch.Size([1, 4]) torch.Size([1, 4])\n",
      "torch.Size([1, 4]) torch.Size([1, 4])\n",
      "torch.Size([1, 4]) torch.Size([1, 4])\n",
      "tensor(64.3966, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs= sentence2tensor(pairs[5000][0],english)\n",
    "outs= sentence2tensor(pairs[5000][1],chinese)\n",
    "print(inputs)\n",
    "encoder_outs=torch.zeros((50,1,1,4),dtype=torch.float)\n",
    "for i in range(inputs.shape[0]):\n",
    "    encoder_outs[i],hidden=ecncoder.forward(inputs[i],ecncoder.init_hidden())\n",
    "\n",
    "loss=0\n",
    "for i in range(outs.shape[0]):\n",
    "    out,de_hid=atten_decorder.forward(torch.tensor([0],dtype=torch.long),hidden,encoder_outs)\n",
    "    loss+=loss_f(out,outs[i])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    for p in pairs:\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        inputs= sentence2tensor(p[0])\n",
    "        target= sentence2tensor(p[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs(path='F:/Github/machine_learn_record/pytorch/data/cmn.txt'):\n",
    "    file=open(path,encoding='utf-8')\n",
    "    content=file.read()\n",
    "    pairs=[]\n",
    "    for p in content.split('\\n'):\n",
    "        temp=p.split('\\t')\n",
    "        pairs.append(temp)\n",
    "        \n",
    "    #调整英语中的符号\n",
    "    for p in pairs:\n",
    "        es=p[0].lower().strip()\n",
    "        es= re.sub(r\"([.!?])\", r\" \\1\", es)\n",
    "        es = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", es)\n",
    "        p[0]=es\n",
    "    #删除最后一行\n",
    "    pairs.pop()\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs=read_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self,name):\n",
    "        self.name=name\n",
    "        self.index2word={}\n",
    "        self.word2index={0: \"SOS\", 1: \"EOS\"}\n",
    "        self.word2count={}\n",
    "        self.n_word=2\n",
    "    \n",
    "    def add_sentence(self,sentence):\n",
    "        if self.name=='en':\n",
    "            for w in sentence.split(' '):\n",
    "                self.add_word(w)\n",
    "        else:\n",
    "            for w in sentence:\n",
    "                self.add_word(w)\n",
    "                \n",
    "    def add_word(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.index2word[self.n_word]=word\n",
    "            self.word2index[word]=self.n_word\n",
    "            self.word2count[word]=1\n",
    "            self.n_word+=1\n",
    "        else:\n",
    "            self.word2count[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese=Lang('cn')\n",
    "english=Lang('en')\n",
    "for p in pairs:\n",
    "    chinese.add_sentence(p[1])\n",
    "    english.add_sentence(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2tensor(sentence,lang):\n",
    "    idxs=[]\n",
    "    if lang.name=='en':\n",
    "        for w in sentence.split(' '):\n",
    "            idxs.append(lang.word2index[w])\n",
    "    else:\n",
    "        for w in sentence:\n",
    "            idxs.append(lang.word2index[w])\n",
    "    tnr=torch.tensor(idxs,dtype=torch.long)\n",
    "    return tnr.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['please leave .', '請你離開。']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([158, 100,   3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2tensor('please leave .',english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([536,   4, 537,  65,   3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2tensor('請你離開。',chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
